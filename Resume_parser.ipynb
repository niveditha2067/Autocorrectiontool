{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZMbtEQfP-95",
        "outputId": "26ed8d3a-e701-4f89-ffdd-9e8e90428482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (40.0.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n"
          ]
        }
      ],
      "source": [
        "pip install pdfminer.six  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install doc2text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR4Icj8SQK0E",
        "outputId": "23ebf92a-44fa-4af1-a73c-56e90a35993a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting doc2text\n",
            "  Downloading doc2text-0.2.4.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from doc2text) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from doc2text) (8.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from doc2text) (1.10.1)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting mime\n",
            "  Downloading mime-0.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from mime->doc2text) (0.18.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract->doc2text) (23.1)\n",
            "Building wheels for collected packages: doc2text, mime\n",
            "  Building wheel for doc2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for doc2text: filename=doc2text-0.2.4-py2.py3-none-any.whl size=8423 sha256=2bfa8c5e7690d706e0c1591d43c5ebfeb5dc887a0115fbc27ae90006fd943775\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/4c/94/fea123c34545f65778983e76ba5dcb4bb2d30cbb90ab6481f7\n",
            "  Building wheel for mime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mime: filename=mime-0.1.0-py3-none-any.whl size=39039 sha256=9b708025d21a998105697607c2470295e8fb8337fb6afcf3598c1b8b5ca03d03\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/c5/b5/d1d1473adc0e57f65151363a1a84489d41101990153850c439\n",
            "Successfully built doc2text mime\n",
            "Installing collected packages: pytesseract, PyPDF2, mime, doc2text\n",
            "Successfully installed PyPDF2-3.0.1 doc2text-0.2.4 mime-0.1.0 pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requires.io\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKAJ05gLQT8M",
        "outputId": "b4354295-a6dc-48ac-ddf7-92789e4ffbdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requires.io\n",
            "  Downloading requires.io-0.2.6-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requires.io) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requires.io) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requires.io) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requires.io) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requires.io) (1.26.15)\n",
            "Installing collected packages: requires.io\n",
            "Successfully installed requires.io-0.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "import io\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as fh:\n",
        "        # iterate over all pages of PDF document\n",
        "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
        "            # creating a resoure manager\n",
        "            resource_manager = PDFResourceManager()\n",
        "            \n",
        "            # create a file handle\n",
        "            fake_file_handle = io.StringIO()\n",
        "            \n",
        "            # creating a text converter object\n",
        "            converter = TextConverter(\n",
        "                                resource_manager, \n",
        "                                fake_file_handle, \n",
        "                                codec='utf-8', \n",
        "                                laparams=LAParams()\n",
        "                        )\n",
        "\n",
        "            # creating a page interpreter\n",
        "            page_interpreter = PDFPageInterpreter(\n",
        "                                resource_manager, \n",
        "                                converter\n",
        "                            )\n",
        "\n",
        "            # process current page\n",
        "            page_interpreter.process_page(page)\n",
        "            # extract text\n",
        "            text = fake_file_handle.getvalue()\n",
        "            yield text\n",
        "\n",
        "            # close open handles\n",
        "            converter.close()\n",
        "            fake_file_handle.close()\n",
        "\n",
        "# calling above function and extracting text\n",
        "text = '' \n",
        "for page in extract_text_from_pdf('ResumePDF.pdf'):\n",
        "    text += ' ' + page\n"
      ],
      "metadata": {
        "id": "NLNCeMaDQfGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install doc2text\n",
        "import doc2text\n",
        "\n",
        "def extract_text_from_doc(doc_path):\n",
        "    temp = doc2text.process(\"ResumePDF.pdf\")\n",
        "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
        "    return ' '.join(text)\n"
      ],
      "metadata": {
        "id": "MrawanqIQmLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkthRb8vQq94",
        "outputId": "a18df8cb-2694-45d1-e2f6-044573993577"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "-7ov3XjpQ0HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# initialize matcher with a vocab\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def extract_name(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "    \n",
        "    # First name and Last name are always Proper Nouns\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    \n",
        "    matcher.add('NAME', None, *pattern)\n",
        "    \n",
        "    matches = matcher(nlp_text)\n",
        "    \n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_text[start:end]\n",
        "        return span.text\n"
      ],
      "metadata": {
        "id": "Snlfsg9zQ4So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_mobile_number(text):\n",
        "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
        "    \n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "        if len(number) > 10:\n",
        "            return '+' + number\n",
        "        else:\n",
        "            return number\n"
      ],
      "metadata": {
        "id": "BSqvFctAQ8W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_email(email):\n",
        "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
        "    if email:\n",
        "        try:\n",
        "            return email[0].split()[0].strip(';')\n",
        "        except IndexError:\n",
        "            return None\n"
      ],
      "metadata": {
        "id": "AnsFTVZhQ-F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGwAN90WRCA0",
        "outputId": "e8985c97-295e-44d3-86ee-58fddcd5ae06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "noun_chunks = nlp.noun_chunks\n",
        "\n",
        "def extract_skills(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # removing stop words and implementing word tokenization\n",
        "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
        "    \n",
        "    # reading the csv file\n",
        "    data = pd.read_csv(\"skills.csv\") \n",
        "    \n",
        "    # extract values\n",
        "    skills = list(data.columns.values)\n",
        "    \n",
        "    skillset = []\n",
        "    \n",
        "    # check for one-grams (example: python)\n",
        "    for token in tokens:\n",
        "        if token.lower() in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    # check for bi-grams and tri-grams (example: machine learning)\n",
        "    for token in noun_chunks:\n",
        "        token = token.text.lower().strip()\n",
        "        if token in skills:\n",
        "            skillset.append(token)\n",
        "    \n",
        "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n"
      ],
      "metadata": {
        "id": "yjZETTndRIek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n",
        "python -m nltk nltk.download('words')"
      ],
      "metadata": {
        "id": "SlnifDYURN-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load pre-trained model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Grad all general stop words\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Education Degrees\n",
        "EDUCATION = [\n",
        "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
        "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
        "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
        "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
        "        ]\n",
        "\n",
        "def extract_education(resume_text):\n",
        "    nlp_text = nlp(resume_text)\n",
        "\n",
        "    # Sentence Tokenizer\n",
        "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
        "\n",
        "    edu = {}\n",
        "    # Extract education degree\n",
        "    for index, text in enumerate(nlp_text):\n",
        "        for tex in text.split():\n",
        "            # Replace all special symbols\n",
        "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
        "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
        "                edu[tex] = text + nlp_text[index + 1]\n",
        "\n",
        "    # Extract year\n",
        "    education = []\n",
        "    for key in edu.keys():\n",
        "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
        "              if year:\n",
        "            education.append((key, ''.join(year[0])))\n",
        "        else:\n",
        "            education.append(key)\n",
        "    return education"
      ],
      "metadata": {
        "id": "00H2F8TnRgxz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}